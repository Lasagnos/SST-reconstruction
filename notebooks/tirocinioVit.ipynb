{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xarray[complete] netcdf4 h5netcdf\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install scipy\n",
    "# %pip install dask\n",
    "# %pip install tensorflow --user\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Conv2DTranspose, Concatenate, concatenate, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset load and train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "\n",
    "# dataset = np.load('dati/mist/datasets/dataset.npy')\n",
    "# date = np.load('dati/mist/datasets/date.npy')\n",
    "\n",
    "dataset_d = np.load('dati/mist/datasets/dataset_d.npy')\n",
    "dataset_n = np.load('dati/mist/datasets/dataset_n.npy')\n",
    "date_d = np.load('dati/mist/datasets/date_d.npy')\n",
    "date_n = np.load('dati/mist/datasets/date_n.npy')\n",
    "\n",
    "# baseline = np.load('dati/mist/datasets/baseline.npy')\n",
    "baseline_d = np.load('dati/mist/datasets/baseline_d.npy')\n",
    "baseline_n = np.load('dati/mist/datasets/baseline_n.npy')\n",
    "\n",
    "italy_mask = np.load('dati/mist/datasets/italy_mask.npy')\n",
    "data_min = np.load('dati/mist/datasets/data_min.npy')\n",
    "data_max = np.load('dati/mist/datasets/data_max.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the day and the night dataset into training and testing sets\n",
    "\n",
    "train_indices_d, temp_indices_d = train_test_split(np.arange(dataset_d.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_d, test_indices_d = train_test_split(temp_indices_d, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_d = dataset_d[train_indices_d]\n",
    "x_val_d = dataset_d[val_indices_d]\n",
    "x_test_d = dataset_d[test_indices_d]\n",
    "\n",
    "dates_train_d = date_d[train_indices_d]\n",
    "dates_val_d = date_d[val_indices_d]\n",
    "dates_test_d = date_d[test_indices_d]\n",
    "\n",
    "\n",
    "train_indices_n, temp_indices_n = train_test_split(np.arange(dataset_n.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_n, test_indices_n = train_test_split(temp_indices_n, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_n = dataset_n[train_indices_n]\n",
    "x_val_n = dataset_n[val_indices_n]\n",
    "x_test_n = dataset_n[test_indices_n]\n",
    "\n",
    "dates_train_n = date_n[train_indices_n]\n",
    "dates_val_n = date_n[val_indices_n]\n",
    "dates_test_n = date_n[test_indices_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss, Metrics and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    clear_mask = y_true[:,:,:,1:2]      # 0 for land/clouds, 1 for clear sea\n",
    "    y_true = y_true[:,:,:,0:1]          # The true SST values. Obfuscated areas are already converted to 0\n",
    "    \n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    clear_masked_error = squared_error * clear_mask\n",
    "\n",
    "    # Calculate the mean of the masked errors\n",
    "    clear_loss = tf.reduce_mean(clear_masked_error)     # The final loss\n",
    "\n",
    "    return clear_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClearMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    clear_mask = y_true[:,:,:,1:2]  # 0 for land/clouds, 1 for clear sea\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    clear_masked_error = squared_error * clear_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    clr_metric = tf.reduce_sum(clear_masked_error) / tf.reduce_sum(clear_mask)\n",
    "\n",
    "    #loss = clear_loss\n",
    "    return clr_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArtificialMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)    # Was getting an error because of the different types: y_true in the metrics is float64 instead of the normal float32\n",
    "\n",
    "    artificial_mask = y_true[:,:,:,2:3]  # 1 for artificial clouds, 0 for the rest\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over artificially clouded areas\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    artificial_masked_error = squared_error * artificial_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    art_metric = tf.reduce_sum(artificial_masked_error) / tf.reduce_sum(artificial_mask)\n",
    "\n",
    "    return art_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "loss = customLoss\n",
    "metrics = [ClearMetric, ArtificialMetric]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "steps_per_epoch = min(100, len(x_train_d) // batch_size)\n",
    "validation_steps = 20\n",
    "testing_steps = 20\n",
    "\n",
    "input_shape = (256, 256, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline generator function\n",
    "\n",
    "def baseline_generator(batch_size, data_d, data_n, dates_d, dates_n, dayChance=0.5):\n",
    "    while True:\n",
    "        batch_x = np.zeros((batch_size, 256, 256, 4))\n",
    "        batch_y = np.zeros((batch_size, 256, 256, 3))\n",
    "\n",
    "        #Randomly choose between day and night dataset\n",
    "        (dataset, date, baseline) = (data_d, dates_d, baseline_d) if np.random.rand() < dayChance else (data_n, dates_n, baseline_n)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Choose a random index as the current day, and 3 random indices\n",
    "            i, r1, r2, r3= np.random.randint(0, dataset.shape[0], 4)\n",
    "\n",
    "            # Extract the image and mask from the current day, and the masks from the other days\n",
    "            image_current = np.nan_to_num(dataset[i], nan=0)\n",
    "            mask_current = np.isnan(dataset[i])\n",
    "            mask_r1 = np.isnan(dataset[r1])\n",
    "            mask_r2 = np.isnan(dataset[r2])\n",
    "            mask_r3 = np.isnan(dataset[r3])\n",
    "\n",
    "            # Perform OR operation between masks\n",
    "            mask_or_r1 = np.logical_or(mask_current, mask_r1)\n",
    "            mask_or_r2 = np.logical_or(mask_current, mask_r2)\n",
    "            mask_or_r3 = np.logical_or(mask_current, mask_r3)\n",
    "            #choose the middle mask\n",
    "            masks = [mask_or_r1, mask_or_r2, mask_or_r3]\n",
    "            masks.sort(key=np.sum)\n",
    "            artificial_mask = masks[1] # The mask with the medium amount of coverage\n",
    "\n",
    "            # Apply the amplified mask to the current day's image\n",
    "            image_masked = np.where(artificial_mask, 0, image_current)\n",
    "            \n",
    "            # Convert the current date to a datetime object using pandas\n",
    "            date_series = pd.to_datetime(date[i], unit='D', origin='unix')\n",
    "            day_of_year = date_series.dayofyear\n",
    "\n",
    "            # Fix masks before they are used in the loss and metric functions\n",
    "            artificial_mask = np.logical_xor(artificial_mask, mask_current)  # 1 for artificially obfuscated, 0 for the rest\n",
    "            mask_current = np.logical_not(mask_current) # 1 for clear sea, 0 for land/clouds\n",
    "            \n",
    "            # Create batch_x and batch_y\n",
    "            batch_x[b, ..., 0] = image_masked               #artificially cloudy image\n",
    "            batch_x[b, ..., 1] = mask_current               #real mask\n",
    "            batch_x[b, ..., 2] = italy_mask                 #land-sea mask\n",
    "            batch_x[b, ..., 3] = baseline[day_of_year - 1]  #baseline values for the current day (day_of_year starts from 1)\n",
    "\n",
    "            batch_y[b, ..., 0] = image_current              #real image\n",
    "            batch_y[b, ..., 1] = mask_current               #real mask\n",
    "            batch_y[b, ..., 2] = artificial_mask            #artificial mask used for the input\n",
    "        \n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generators\n",
    "\n",
    "train_gen = baseline_generator(batch_size, x_train_d, x_train_n, dates_train_d, dates_train_n)\n",
    "val_gen = baseline_generator(batch_size, x_val_d, x_val_n, dates_val_d, dates_val_n)\n",
    "test_gen = baseline_generator(batch_size, x_test_d, x_test_n, dates_test_d, dates_test_n)\n",
    "\n",
    "# Test generator that returns dates\n",
    "#test_gen_dates = gen_qual_dates(batch_size, x_test_d, x_test_n, q_test_d, q_test_n, dates_test_d, dates_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "\n",
    "x,y = next(train_gen)\n",
    "r = np.random.randint(0, batch_size)    # Choose a random image from the batch\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the x data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x[r, :, :, 0], cmap='jet')\n",
    "plt.title(\"x_0 (model input)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the y data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(y[r, :, :, 0], cmap='jet')\n",
    "plt.title(\"y_0 (ground truth)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Information about the data\n",
    "#print(np.isnan(x).any())\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"min of all x:\", np.min(x[..., 0]))\n",
    "print(\"max of all x:\", np.max(x[..., 0]))\n",
    "print(\"min of this x:\", np.min(x[r, :, :, 0]))\n",
    "print(\"max of this x:\", np.max(x[r, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the model's hyperparameters\n",
    "\n",
    "#input_shape = (256, 256, 4)\n",
    "\n",
    "image_size = 256  # We'll resize input images to this size\n",
    "patch_size = 8  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 128    \n",
    "num_heads = 6   # was 6\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 6  # was 6\n",
    "mlp_head_units = [\n",
    "    256,\n",
    "    128,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Transformer model \n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = tf.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = tf.image.extract_patches(images, sizes=[1,8,8,1], strides=[1,8,8,1], rates=[1,1,1,1], padding='VALID')\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.expand_dims(\n",
    "            tf.experimental.numpy.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config\n",
    "\n",
    "# only used in postprocessing\n",
    "def ResidualBlock(width):\n",
    "            def apply(x):\n",
    "                input_width = x.shape[3]\n",
    "                if input_width == width:\n",
    "                    residual = x\n",
    "                else:\n",
    "                    residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "                #x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "                x = layers.LayerNormalization(axis=-1,center=True, scale=True)(x)\n",
    "                x = layers.Conv2D(\n",
    "                    width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
    "                )(x)\n",
    "                x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "                x = layers.Add()([x, residual])\n",
    "                return x\n",
    "\n",
    "            return apply\n",
    "\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_vit():\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=mlp_head_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    out = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # Spatial reshape of the patches\n",
    "    out = layers.Reshape((32, 32, 128))(out)\n",
    "    # Residual block\n",
    "    out = ResidualBlock(256)(out)\n",
    "    # Upsample\n",
    "    out = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(out)\n",
    "    out = ResidualBlock(128)(out)\n",
    "    # Upsample while reducing channel size\n",
    "    out = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(out)\n",
    "    out = ResidualBlock(64)(out)\n",
    "    out = layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='tanh')(out) #tanh\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, create it and print the summary\n",
    "\n",
    "model = create_vit()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with custom loss function\n",
    "opt = Adam(learning_rate=lr)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WEIGHTS\n",
    "# model.load_weights('weights/visualTransformer.weights.h5')   # does not work on local machine\n",
    "model.load_weights('weights/visualTransformer.h5')\n",
    "\n",
    "# SAVE WEIGHTS\n",
    "#model.save_weights('weights/visualTransformer.h5')  # execute remotely after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "#history = model.fit(train_gen, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=val_gen, validation_steps=validation_steps, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment on Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop error calculation over tot batches\n",
    "\n",
    "# Initialize lists to store the average errors, maximum errors, and variances\n",
    "avg_errors_list = []\n",
    "avg_max_errors_list = []\n",
    "var_max_errors_list = []\n",
    "\n",
    "# Generate and evaluate tot batches\n",
    "tot = 100\n",
    "for _ in range(tot):\n",
    "    # Generate a batch\n",
    "    x_true, y_true = next(test_gen)\n",
    "    predictions = model.predict(x_true)\n",
    "\n",
    "    # Denormalize\n",
    "    predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "    true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    # Calculate the errors\n",
    "    clearMask = y_true[..., 1]\n",
    "    errors = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "\n",
    "    # Calculate the average and maximum error for each image in the batch\n",
    "    avg_errors = np.nanmean(errors, axis=(1, 2))\n",
    "    max_errors = np.nanmax(errors, axis=(1, 2))\n",
    "\n",
    "    # Add the average error, average maximum error, and variance of maximum errors to the lists\n",
    "    avg_errors_list.append(np.mean(avg_errors))\n",
    "    avg_max_errors_list.append(np.mean(max_errors))\n",
    "    var_max_errors_list.append(np.var(max_errors))\n",
    "\n",
    "# Print the average, average maximum, and variance of maximums calculated over tot batches\n",
    "print(f\"Average error over {tot} batches:\", np.mean(avg_errors_list))\n",
    "print(f\"Average maximum error over {tot} batches:\", np.mean(avg_max_errors_list))\n",
    "print(f\"Variance of maximum errors over {tot} batches:\", np.mean(var_max_errors_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to show loss and metrics\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "x_true, y_true = next(test_gen)\n",
    "results = model.evaluate(x_true, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Generate predictions. This generates a batch of data.\n",
    "x_true, y_true = next(test_gen)\n",
    "print(\"x_true shape:\", x_true.shape)\n",
    "print(\"y_true shape:\", y_true.shape)\n",
    "print(\"is there a Nan in x?\", np.isnan(x_true).any())\n",
    "print(\"is there a Nan in y?\", np.isnan(y_true).any())\n",
    "\n",
    "predictions = model.predict(x_true)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"y's min:\", np.min(y_true[:, :, :, 0]))\n",
    "print(\"y's max:\", np.max(y_true[:, :, :, 0]))\n",
    "print(\"x's min:\", np.min(predictions[:, :, :, 0]))\n",
    "print(\"x's max:\", np.max(predictions[:, :, :, 0]))\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "evalx = model.evaluate(x_true, y_true)\n",
    "print(\"evalx: \", evalx)\n",
    "xloss = customLoss(y_true, predictions)\n",
    "print(\"xloss: \", xloss)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "#get the coordinates of min and max values in a single prediction. This is to check if the model is predicting the same values as the true ones\n",
    "coordxmin = np.argmin(predictions[0, :, :, 0])\n",
    "coordxmax = np.argmax(predictions[0, :, :, 0])\n",
    "print(\"first x's min:\", coordxmin%256, coordxmin//256, np.nanmin(predictions[0, :, :, 0]))\n",
    "print(\"first x's max:\", coordxmax%256, coordxmax//256, np.nanmax(predictions[0, :, :, 0]))\n",
    "print(\"predictions in coordxmin:\", predictions[0, coordxmin//256, coordxmin%256, 0])\n",
    "print(\"predictions in coordxmax:\", predictions[0, coordxmax//256, coordxmax%256, 0])\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Plot the predictions and true values\n",
    "\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # Plot the true value\n",
    "    plt.subplot(1, 3, 1)\n",
    "    mask_overlay = np.where(y_true[i, :, :, 1], y_true[i, :, :, 0], np.nan)\n",
    "    plt.imshow(mask_overlay, cmap='jet', vmin=-1, vmax=1)\n",
    "    plt.title(\"y_0 (Ground Truth)\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot the prediction with the land mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    masked_prediction = np.where(italy_mask, predictions[i, :, :, 0], np.nan)\n",
    "    plt.imshow(masked_prediction, cmap='jet', vmin=-1, vmax=1)\n",
    "    plt.title(\"Prediction with Land Mask\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # # Plot the predicted 'pure' value \n",
    "    # plt.subplot(1, 3, 3)\n",
    "    # plt.imshow(predictions[i], cmap='jet', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Unmasked prediction (DEBUG)\")\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE for the predictions and the baseline\n",
    "\n",
    "batch_x, batch_y = next(test_gen)\n",
    "predictions = model.predict(batch_x)\n",
    "\n",
    "filter_mask = batch_y[..., 1].astype(bool)    # We filter out the land and cloud data\n",
    "\n",
    "\n",
    "# Calculate the MSE for the predictions and the baseline, and the average MSEs in that batch. Only consider the ocean data.\n",
    "mse_predictions = [mean_squared_error(batch_y[i, :, :, 0][filter_mask[i]], predictions[i, :, :, 0][filter_mask[i]]) for i in range(batch_size)]\n",
    "mse_baseline = [mean_squared_error(batch_y[i, :, :, 0][filter_mask[i]], batch_x[i, :, :, 3][filter_mask[i]]) for i in range(batch_size)]\n",
    "print('MSE for predictions:', mse_predictions)\n",
    "print('MSE for baseline:', mse_baseline)\n",
    "\n",
    "avg_mse_predictions = np.mean(mse_predictions)\n",
    "avg_mse_baseline = np.mean(mse_baseline)\n",
    "print('Average MSE for predictions:', avg_mse_predictions)\n",
    "print('Average MSE for baseline:', avg_mse_baseline)\n",
    "\n",
    "# Plot the MSEs\n",
    "indices = np.arange(batch_size + 1)   # Add 1 to the batch size to include the average MSEs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(indices[:-1] - 0.2, mse_predictions, width=0.4, label='Predictions')\n",
    "ax.bar(indices[:-1] + 0.2, mse_baseline, width=0.4, label='Baseline')\n",
    "ax.bar(batch_size - 0.2, avg_mse_predictions, width=0.4, color='blue', label='Avg of Predictions')\n",
    "ax.bar(batch_size + 0.2, avg_mse_baseline, width=0.4, color='red', label='Avg of Baseline')\n",
    "\n",
    "ax.set_xlabel('Batch Index')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE for Predictions and Baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(\"mseGraph.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
