{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Conv2DTranspose, Concatenate, concatenate, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "\n",
    "date_d = np.load('dati/mist/datasets/date_d.npy')\n",
    "date_n = np.load('dati/mist/datasets/date_n.npy')\n",
    "dataset_d = np.load('dati/mist/datasets/dataset_d.npy')\n",
    "dataset_n = np.load('dati/mist/datasets/dataset_n.npy')\n",
    "\n",
    "# abs_dataset_d = np.load('dati/mist/datasets/abs_dataset_d.npy')\n",
    "# abs_dataset_n = np.load('dati/mist/datasets/abs_dataset_n.npy')\n",
    "# training_baseline_d = np.load('dati/mist/datasets/training_baseline_d.npy')\n",
    "# training_baseline_n = np.load('dati/mist/datasets/training_baseline_n.npy')\n",
    "\n",
    "italy_mask = np.load('dati/mist/datasets/italy_mask.npy')\n",
    "data_min = np.load('dati/mist/datasets/data_min.npy')\n",
    "data_max = np.load('dati/mist/datasets/data_max.npy')\n",
    "\n",
    "abs_new2_baseline_d = np.load('dati/mist/datasets/abs_new2_baseline_d.npy')\n",
    "abs_new2_baseline_n = np.load('dati/mist/datasets/abs_new2_baseline_n.npy')\n",
    "abs_new2_baseline_d = 2 * ((abs_new2_baseline_d - data_min) / (data_max - data_min)) - 1\n",
    "abs_new2_baseline_n = 2 * ((abs_new2_baseline_n - data_min) / (data_max - data_min)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the day and the night dataset into training and testing sets\n",
    "\n",
    "train_indices_d, temp_indices_d = train_test_split(np.arange(dataset_d.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_d, test_indices_d = train_test_split(temp_indices_d, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_d = dataset_d[train_indices_d]\n",
    "x_val_d = dataset_d[val_indices_d]\n",
    "x_test_d = dataset_d[test_indices_d]\n",
    "\n",
    "dates_train_d = date_d[train_indices_d]\n",
    "dates_val_d = date_d[val_indices_d]\n",
    "dates_test_d = date_d[test_indices_d]\n",
    "\n",
    "\n",
    "train_indices_n, temp_indices_n = train_test_split(np.arange(dataset_n.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_n, test_indices_n = train_test_split(temp_indices_n, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_n = dataset_n[train_indices_n]\n",
    "x_val_n = dataset_n[val_indices_n]\n",
    "x_test_n = dataset_n[test_indices_n]\n",
    "\n",
    "dates_train_n = date_n[train_indices_n]\n",
    "dates_val_n = date_n[val_indices_n]\n",
    "dates_test_n = date_n[test_indices_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    real_mask = y_true[:,:,:,1:2]        # 0 for land/clouds, 1 for sea\n",
    "    y_true = y_true[:,:,:,0:1]          # The true SST values. Obfuscated areas are already converted to 0\n",
    "    \n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    masked_error = squared_error * real_mask\n",
    "\n",
    "    # Calculate the mean of the masked errors\n",
    "    clear_loss = tf.reduce_mean(masked_error)     # The final loss\n",
    "\n",
    "    return clear_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClearMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    art_mask = y_true[:,:,:,2:3]   # 0 for land/clouds + artificials, 1 for clear, untouched sea\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    clear_masked_error = squared_error * art_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    clr_metric = tf.reduce_sum(clear_masked_error) / tf.reduce_sum(art_mask)\n",
    "\n",
    "    #loss = clear_loss\n",
    "    return clr_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArtificialMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)    # Was getting an error because of the different types: y_true in the metrics is float64 instead of the normal float32\n",
    "\n",
    "    real_mask = y_true[:,:,:,1:2]  # 0 for land/clouds, 1 for clear sea\n",
    "    art_mask = y_true[:,:,:,2:3]  # 0 for land/clouds + artificials, 1 for clear sea\n",
    "    added_mask = real_mask - art_mask  # 1 only for hidden sea, 0 for land/clouds and visible sea\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over artificially clouded areas\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    artificial_masked_error = squared_error * added_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    art_metric = tf.reduce_sum(artificial_masked_error) / tf.reduce_sum(added_mask)\n",
    "\n",
    "    return art_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSEMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    real_mask = y_true[:,:,:,1:2]  # 0 for land/clouds, 1 for clear sea\n",
    "    art_mask = y_true[:,:,:,2:3]  # 0 for land/clouds + artificials, 1 for clear sea\n",
    "    added_mask = real_mask - art_mask  # 1 only for hidden sea, 0 for land/clouds and visible sea\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Denormalize the predictions and the true values\n",
    "    y_pred_denorm = ((y_pred + 1) / 2) * (data_max - data_min) + data_min\n",
    "    y_true_denorm = ((y_true + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    # Calculate the squared error only over hidden sea\n",
    "    squared_error = tf.square(y_true_denorm - y_pred_denorm)\n",
    "    hidden_masked_error = squared_error * added_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    mse_metric = tf.reduce_sum(hidden_masked_error) / tf.reduce_sum(added_mask)\n",
    "\n",
    "    # Calculate the square root of the mean squared error to get the RMSE\n",
    "    rmse_loss = tf.sqrt(mse_metric)\n",
    "\n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "loss = customLoss\n",
    "metrics = [ClearMetric, ArtificialMetric, RMSEMetric]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=epochs, verbose=1, restore_best_weights=True)\n",
    "\n",
    "#steps_per_epoch = min(100, len(x_train_d) // batch_size)\n",
    "# validation_steps = 20\n",
    "# testing_steps = 20\n",
    "steps_per_epoch = len(x_train_d) // batch_size\n",
    "validation_steps = len(x_val_d) // batch_size\n",
    "testing_steps = len(x_test_d) // batch_size\n",
    "\n",
    "input_shape = (256, 256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function\n",
    "def generator(batch_size, data_d, data_n, dates_d, dates_n, dayChance=0.5):\n",
    "    i = 0   # Counter for the dataset. We will use the whole dataset, one batch at a time\n",
    "    while True:\n",
    "        batch_x = np.zeros((batch_size, 256, 256, 4))\n",
    "        batch_y = np.zeros((batch_size, 256, 256, 3))\n",
    "        batch_dates = []\n",
    "\n",
    "        #Randomly choose between day and night dataset\n",
    "        isDay = np.random.rand() < dayChance\n",
    "        (dataset, date, baseline) = (data_d, dates_d, abs_new2_baseline_d) if isDay else (data_n, dates_n, abs_new2_baseline_n)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Choose a random index as the current day, and 3 random indices\n",
    "            r1, r2, r3= np.random.randint(0, dataset.shape[0], 3)\n",
    "\n",
    "            # Extract the image and mask from the current day, and the masks from the other days\n",
    "            image_current = np.nan_to_num(dataset[i], nan=0)\n",
    "            mask_current = np.isnan(dataset[i])\n",
    "            mask_r1 = np.isnan(dataset[r1])\n",
    "            mask_r2 = np.isnan(dataset[r2])\n",
    "            mask_r3 = np.isnan(dataset[r3])\n",
    "\n",
    "            # Perform OR operation between masks\n",
    "            mask_or_r1 = np.logical_or(mask_current, mask_r1)\n",
    "            mask_or_r2 = np.logical_or(mask_current, mask_r2)\n",
    "            mask_or_r3 = np.logical_or(mask_current, mask_r3)\n",
    "            #choose the middle mask\n",
    "            masks = [mask_or_r1, mask_or_r2, mask_or_r3]\n",
    "            masks.sort(key=np.sum)\n",
    "            artificial_mask = masks[1] # The mask with the medium amount of coverage\n",
    "\n",
    "            # Apply the amplified mask to the current day's image\n",
    "            image_masked = np.where(artificial_mask, 0, image_current)\n",
    "\n",
    "            # Convert the current date to a datetime object using pandas\n",
    "            date_series = pd.to_datetime(date[i], unit='D', origin='unix')\n",
    "            day_of_year = date_series.dayofyear\n",
    "            #Before they are added to the batch_dates list, the dates are converted to strings (no time)\n",
    "            date_series = date_series.strftime('%Y-%m-%d')\n",
    "            batch_dates.append(date_series) # Append the current date to the batch_dates list\n",
    "\n",
    "            #avg temp of the current day\n",
    "            image_masked_nan = np.where(artificial_mask, np.nan, image_current)\n",
    "            if(np.isnan(image_masked_nan).all()):\n",
    "                tuned_baseline = baseline[day_of_year - 1]\n",
    "            else:\n",
    "                avg_temp_real = np.nanmean(image_masked_nan)\n",
    "                avg_temp_baseline = np.nanmean(np.where(artificial_mask, np.nan, baseline[day_of_year - 1]))\n",
    "                tuned_baseline = baseline[day_of_year - 1] + avg_temp_real - avg_temp_baseline  # Adjust the baseline to match the average temperature of the current day\n",
    "            tuned_baseline = np.where(italy_mask, tuned_baseline, 0)    # Apply the land-sea mask\n",
    "\n",
    "            # Fix masks before they are used in the loss and metric functions\n",
    "            artificial_mask = np.logical_not(artificial_mask)  # 1 for clear sea, 0 for land/clouds and artificial clouds\n",
    "            mask_current = np.logical_not(mask_current) # 1 for clear sea, 0 for land/clouds\n",
    "\n",
    "            # Increment the index\n",
    "            i += 1\n",
    "            if i >= dataset.shape[0] - 1:\n",
    "                i = 0\n",
    "            \n",
    "            # Create batch_x and batch_y\n",
    "            batch_x[b, ..., 0] = image_masked               #artificially cloudy image\n",
    "            batch_x[b, ..., 1] = artificial_mask            #artificial mask\n",
    "            batch_x[b, ..., 2] = italy_mask                 #land-sea mask\n",
    "            batch_x[b, ..., 3] = tuned_baseline             #tuned baseline\n",
    "\n",
    "            batch_y[b, ..., 0] = image_current              #real image\n",
    "            batch_y[b, ..., 1] = mask_current               #real mask\n",
    "            batch_y[b, ..., 2] = artificial_mask            #artificial mask used for the input\n",
    "        \n",
    "        dn_flag = 'day' if isDay else 'night'\n",
    "        yield batch_x, batch_y, batch_dates, dn_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generators\n",
    "\n",
    "train_gen = generator(batch_size, x_train_d, x_train_n, dates_train_d, dates_train_n)\n",
    "val_gen = generator(batch_size, x_val_d, x_val_n, dates_val_d, dates_val_n)\n",
    "test_gen = generator(batch_size, x_test_d, x_test_n, dates_test_d, dates_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "\n",
    "x,y,date,dn_flag = next(train_gen)\n",
    "r = np.random.randint(0, batch_size)    # Choose a random image from the batch\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "vmin = min(np.min(x[r, :, :, 0]), np.min(y[r, :, :, 0]))\n",
    "vmax = max(np.max(x[r, :, :, 0]), np.max(y[r, :, :, 0]))\n",
    "\n",
    "# Plot the x data\n",
    "plt.subplot(2, 2, 1)\n",
    "# masked_x = np.where(x[r, :, :, 1], x[r, :, :, 0], np.nan)\n",
    "# plt.imshow(masked_x, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "plt.imshow(x[r, :, :, 0], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "plt.title(\"x_0 (model input)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the y data\n",
    "plt.subplot(2, 2, 2)\n",
    "# masked_y = np.where(y[r, :, :, 1], y[r, :, :, 0], np.nan)\n",
    "# plt.imshow(masked_y, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "plt.imshow(y[r, :, :, 0], cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "plt.title(\"y_0 (ground truth)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Information about the data\n",
    "print(f\"Showing day {date[r]}, from the '{dn_flag}' dataset\")\n",
    "#print(np.isnan(x).any())\n",
    "print(\"\\nx.shape:\", x.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "# print(\"min of all x:\", np.min(x[..., 0]))\n",
    "# print(\"max of all x:\", np.max(x[..., 0]))\n",
    "print(\"min in x:\", np.min(x[r, :, :, 0]))\n",
    "print(\"max in x:\", np.max(x[r, :, :, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model with residual blocks\n",
    "\n",
    "def ResidualBlock(depth):\n",
    "    def apply(x):\n",
    "        input_depth = x.shape[3]    # Get the number of channels from the channels dimension\n",
    "        if input_depth == depth:    # It's already the desired channel number\n",
    "            residual = x\n",
    "        else:                       # Adjust the number of channels with a 1x1 convolution\n",
    "            residual = Conv2D(depth, kernel_size=1)(x)\n",
    "\n",
    "        x = BatchNormalization(center=False, scale=False)(x)    \n",
    "        x = Conv2D(depth, kernel_size=3, padding=\"same\", activation='swish')(x) \n",
    "        x = Conv2D(depth, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Add()([x, residual])\n",
    "        return x\n",
    "    \n",
    "    return apply\n",
    "\n",
    "\n",
    "def DownBlock(depth, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(depth)(x)\n",
    "            skips.append(x)\n",
    "        x = AveragePooling2D(pool_size=2)(x)    #downsampling\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(depth, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = UpSampling2D(size=2, interpolation=\"bilinear\")(x)   #upsampling\n",
    "        for _ in range(block_depth):\n",
    "            x = Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(depth)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_Unet(image_size, depths, block_depth):\n",
    "    input_images = Input(shape=image_size)  #input layer\n",
    "    \n",
    "    x = Conv2D(depths[0], kernel_size=1)(input_images)  #reduce the number of channels\n",
    "\n",
    "    skips = []  #store the skip connections\n",
    "    \n",
    "    for depth in depths[:-1]:   #downsampling layers\n",
    "        x = DownBlock(depth, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):    #middle layer\n",
    "        x = ResidualBlock(depths[-1])(x)\n",
    "\n",
    "    for depth in reversed(depths[:-1]):   #upsampling layers\n",
    "        x = UpBlock(depth, block_depth)([x, skips])\n",
    "\n",
    "    x = Conv2D(1, kernel_size=1, kernel_initializer=\"zeros\", name = \"output_noise\")(x)  #output layer, no activation function\n",
    "    \n",
    "    return Model(input_images, outputs=x, name=\"UNetInpainter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, create it and print the summary\n",
    "depths = [32, 64, 128, 256, 512]\n",
    "block_depth = 2\n",
    "\n",
    "model = get_Unet(input_shape, depths, block_depth)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with custom loss function\n",
    "opt = Adam(learning_rate=lr)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WEIGHTS\n",
    "model.load_weights('weights/standard.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sampls where the max error is above a certain degree, and point it in the plot\n",
    "\n",
    "# Generate and evaluate tot batches\n",
    "tot = 1 #10\n",
    "for _ in range(tot):\n",
    "    # Generate a batch\n",
    "    x_true, y_true, batch_dates, dn_flag = next(test_gen)\n",
    "    predictions = model.predict(x_true, verbose=0)\n",
    "\n",
    "    x_true1 = x_true.copy()\n",
    "    #x_true1[..., 3] = predictions[..., 0]  # Replace the baseline with the predictions\n",
    "\n",
    "    avg_temp_real = np.nanmean(x_true[..., 0])\n",
    "    avg_temp_pred = np.nanmean(np.where(x_true[..., 1], predictions[..., 0], np.nan))    # Inverse of the generator because the mask is inverted\n",
    "    tuned_pred = predictions[..., 0] + avg_temp_real - avg_temp_pred\n",
    "    tuned_pred = np.where(italy_mask, tuned_pred, 0)    # Apply the land-sea mask\n",
    "\n",
    "    x_true1[..., 3] = tuned_pred   # Use the prediction as the baseline\n",
    "    predictions1 = model.predict(x_true1, verbose=0)\n",
    "\n",
    "    # Denormalize\n",
    "    predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "    true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "    predictions1_denorm = ((predictions1[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    nanbaseline = np.where(italy_mask, x_true[..., 3], np.nan)\n",
    "    baseline_denorm = ((nanbaseline + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    # Get the masks and calculate the errors\n",
    "    realMask = y_true[..., 1]                                   #real mask\n",
    "    hiddenMask = np.not_equal(y_true[..., 1], x_true[..., 1])   #hidden sea\n",
    "    clearMask = x_true[..., 1]                                  #clear sea\n",
    "\n",
    "    errors = np.where(realMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "    errors1 = np.where(realMask, np.abs(predictions1_denorm - true_values_denorm), np.nan)\n",
    "    clear_errors_batch = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "    hidden_errors_batch = np.where(hiddenMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "\n",
    "    # Print the samples where the max error is above a certain degree, and point it in the plot\n",
    "    treshold = 7\n",
    "    #iterate over errors and find the ones above the treshold\n",
    "    for i in range(len(errors)):\n",
    "        print(f\"Day {batch_dates[i]} from the {dn_flag} dataset\")\n",
    "        maxerr = np.nanmax(errors[i])\n",
    "        maxerr_coords = np.unravel_index(np.nanargmax(errors[i]), errors[i].shape)\n",
    "        if maxerr > treshold:\n",
    "            print(f\"Error of {np.nanmax(errors[i])} found in coordinates {maxerr_coords}\")\n",
    "\n",
    "        #plot the image\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # vmin = min(np.nanmin(true_values_denorm[i]), np.nanmin(baseline_denorm))\n",
    "        # vmax = max(np.nanmax(true_values_denorm[i]), np.nanmax(baseline_denorm))\n",
    "        vmin = min(np.nanmin(true_values_denorm[i]), np.nanmin(predictions_denorm[i]), np.nanmin(baseline_denorm[i]))\n",
    "        vmax = max(np.nanmax(true_values_denorm[i]), np.nanmax(predictions_denorm[i]), np.nanmax(baseline_denorm[i]))\n",
    "\n",
    "        plt.subplot(2, 3, 1)\n",
    "        mask_overlay = np.where(y_true[i, :, :, 1], true_values_denorm[i], np.nan)\n",
    "        plt.imshow(mask_overlay, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        plt.title(\"denormalized y_0 (ground truth)\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        masked_input = np.where(x_true[i, :, :, 1], true_values_denorm[i], np.nan)\n",
    "        plt.imshow(masked_input, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        plt.title(\"denormalized x_0 (model input)\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        masked_prediction = np.where(italy_mask, predictions_denorm[i], np.nan)\n",
    "        #plt.scatter(maxerr_coords[1], maxerr_coords[0], c='magenta', s=10)  #mark the maximum error\n",
    "        plt.imshow(masked_prediction, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        plt.title(\"denormalized prediction\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        # plt.subplot(2, 3, 4)\n",
    "        # plt.imshow(baseline_denorm[i], cmap='viridis', vmin=vmin, vmax=vmax)    #, cmap='Purples')\n",
    "        # plt.title(\"denormalized (tuned) baseline\")\n",
    "        # #plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        masked_error = np.where(y_true[i, :, :, 1], errors[i], np.nan)\n",
    "        #plt.scatter(maxerr_coords[1], maxerr_coords[0], c='magenta', s=10)  #mark the maximum error\n",
    "        plt.imshow(errors[i], cmap='hot_r')\n",
    "        plt.title(\"error\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        masked_prediction1 = np.where(italy_mask, predictions1_denorm[i], np.nan)\n",
    "        plt.imshow(masked_prediction1, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        plt.title(\"denormalized prediction1\")\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        masked_error1 = np.where(y_true[i, :, :, 1], errors1[1], np.nan)\n",
    "        plt.imshow(masked_error1, cmap='hot_r')\n",
    "        plt.title(\"prediction difference\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE ERRORS\n",
    "print(\"Evaluating the errors, please wait...\")\n",
    "\n",
    "# Initialize lists to store the errors and the maximum errors\n",
    "all_errors = []\n",
    "max_errors = []         #for each element\n",
    "clear_errors = []\n",
    "clear_max_errors = []   #for each element always visible\n",
    "hidden_errors = []\n",
    "hidden_max_errors = []  #for each element hidden artificially\n",
    "\n",
    "# Generate and evaluate tot batches\n",
    "tot = 100\n",
    "for _ in range(tot):\n",
    "    # Generate a batch\n",
    "    x_true, y_true = next(test_gen)\n",
    "    predictions = model.predict(x_true, verbose=0)\n",
    "\n",
    "    # Denormalize\n",
    "    predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "    true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    # Get the masks and calculate the errors\n",
    "    realMask = y_true[..., 1]                                   #real mask\n",
    "    hiddenMask = np.not_equal(y_true[..., 1], x_true[..., 1])   #hidden sea\n",
    "    clearMask = x_true[..., 1]                                  #clear sea\n",
    "\n",
    "    errors = np.where(realMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "    clear_errors_batch = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "    hidden_errors_batch = np.where(hiddenMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "\n",
    "    # Flatten the errors and add to the list\n",
    "    all_errors.extend(errors.flatten())\n",
    "    max_errors.append(np.nanmax(errors))\n",
    "    clear_errors.extend(clear_errors_batch.flatten())\n",
    "    clear_max_errors.append(np.nanmax(clear_errors_batch))\n",
    "    hidden_errors.extend(hidden_errors_batch.flatten())\n",
    "    hidden_max_errors.append(np.nanmax(hidden_errors_batch))\n",
    "\n",
    "# Convert to numpy array for easier calculations\n",
    "all_errors = np.array(all_errors)\n",
    "max_errors = np.array(max_errors)\n",
    "clear_errors = np.array(clear_errors)\n",
    "clear_max_errors = np.array(clear_max_errors)\n",
    "hidden_errors = np.array(hidden_errors)\n",
    "hidden_max_errors = np.array(hidden_max_errors)\n",
    "\n",
    "# Calculate the metrics over all errors\n",
    "avg_error = np.nanmean(all_errors)\n",
    "max_error = np.nanmax(all_errors)\n",
    "avg_max_error = np.nanmean(max_errors)\n",
    "var_error = np.nanvar(all_errors)\n",
    "rmse_error = np.sqrt(np.nanmean(all_errors**2))\n",
    "# Calculate the metrics over clear errors\n",
    "avg_clear_error = np.nanmean(clear_errors)\n",
    "max_clear_error = np.nanmax(clear_errors)\n",
    "avg_max_clear_error = np.nanmean(clear_max_errors)\n",
    "var_clear_error = np.nanvar(clear_errors)\n",
    "rmse_clear_error = np.sqrt(np.nanmean(clear_errors**2))\n",
    "# Calculate the metrics over hidden errors\n",
    "avg_hidden_error = np.nanmean(hidden_errors)\n",
    "max_hidden_error = np.nanmax(hidden_errors)\n",
    "avg_max_hidden_error = np.nanmean(hidden_max_errors)\n",
    "var_hidden_error = np.nanvar(hidden_errors)\n",
    "rmse_hidden_error = np.sqrt(np.nanmean(hidden_errors**2))\n",
    "\n",
    "# Print the metrics calculated over all elements\n",
    "print(f\"\\nAverage error over all elements:\", avg_error)\n",
    "print(f\"Average maximum error:\", avg_max_error, \", and maximum error:\", max_error)\n",
    "print(f\"Variance of errors over all elements:\", var_error)\n",
    "print(f\"RMSE over all elements:\", rmse_error)\n",
    "\n",
    "# Print the metrics calculated over clear elements\n",
    "print(f\"\\nAverage error over clear elements:\", avg_clear_error)\n",
    "print(f\"Average maximum error over clear elements:\", avg_max_clear_error, \", and maximum error over clear elements:\", max_clear_error)\n",
    "print(f\"Variance of errors over clear elements:\", var_clear_error)\n",
    "print(f\"RMSE over clear elements:\", rmse_clear_error)\n",
    "\n",
    "# Print the metrics calculated over hidden elements\n",
    "print(f\"\\nAverage error over hidden elements:\", avg_hidden_error)\n",
    "print(f\"Average maximum error over hidden elements:\", avg_max_hidden_error, \", and maximum error over hidden elements:\", max_hidden_error)\n",
    "print(f\"Variance of errors over hidden elements:\", var_hidden_error)\n",
    "print(f\"RMSE over hidden elements:\", rmse_hidden_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
